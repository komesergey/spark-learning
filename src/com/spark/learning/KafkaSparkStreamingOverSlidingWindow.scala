package com.spark.learning

import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions.window

/*
    -------------------------------------------
    Batch: 0
    -------------------------------------------
    +------------------------------------------+-----+
    |window                                    |count|
    +------------------------------------------+-----+
    |[2019-02-27 12:40:00, 2019-02-27 12:50:00]|1    |
    |[2019-02-27 12:50:00, 2019-02-27 13:00:00]|2    |
    |[2019-02-27 12:45:00, 2019-02-27 12:55:00]|2    |
    |[2019-02-27 12:35:00, 2019-02-27 12:45:00]|1    |
    +------------------------------------------+-----+

    -------------------------------------------
    Batch: 1
    -------------------------------------------
    +------------------------------------------+-----+
    |window                                    |count|
    +------------------------------------------+-----+
    |[2019-02-27 12:40:00, 2019-02-27 12:50:00]|1    |
    |[2019-02-27 14:20:00, 2019-02-27 14:30:00]|1    |
    |[2019-02-27 12:50:00, 2019-02-27 13:00:00]|2    |
    |[2019-02-27 12:45:00, 2019-02-27 12:55:00]|2    |
    |[2019-02-27 12:35:00, 2019-02-27 12:45:00]|1    |
    |[2019-02-27 14:25:00, 2019-02-27 14:35:00]|1    |
    +------------------------------------------+-----+

    -------------------------------------------
    Batch: 2
    -------------------------------------------
    +------------------------------------------+-----+
    |window                                    |count|
    +------------------------------------------+-----+
    |[2019-02-27 12:40:00, 2019-02-27 12:50:00]|1    |
    |[2019-02-27 14:20:00, 2019-02-27 14:30:00]|2    |
    |[2019-02-27 12:50:00, 2019-02-27 13:00:00]|2    |
    |[2019-02-27 12:45:00, 2019-02-27 12:55:00]|2    |
    |[2019-02-27 12:35:00, 2019-02-27 12:45:00]|1    |
    |[2019-02-27 14:25:00, 2019-02-27 14:35:00]|2    |
    +------------------------------------------+-----+

    -------------------------------------------
    Batch: 3
    -------------------------------------------
    +------------------------------------------+-----+
    |window                                    |count|
    +------------------------------------------+-----+
    |[2019-02-27 12:40:00, 2019-02-27 12:50:00]|1    |
    |[2019-02-27 14:20:00, 2019-02-27 14:30:00]|2    |
    |[2019-02-27 12:50:00, 2019-02-27 13:00:00]|2    |
    |[2019-02-27 14:35:00, 2019-02-27 14:45:00]|1    |
    |[2019-02-27 12:45:00, 2019-02-27 12:55:00]|2    |
    |[2019-02-27 12:35:00, 2019-02-27 12:45:00]|1    |
    |[2019-02-27 14:30:00, 2019-02-27 14:40:00]|1    |
    |[2019-02-27 14:25:00, 2019-02-27 14:35:00]|2    |
    +------------------------------------------+-----+
 */
object KafkaSparkStreamingOverSlidingWindow {
  def main(args: Array[String]): Unit = {
    Logger.getLogger("org").setLevel(Level.ERROR)
    val spark = SparkSession.builder.appName("KafkaSparkStreaming")
      .config("spark.master", "local")
      .config("driver-memory", "10G")
      .config("executor-memory", "15G")
      .config("executor-cores", "8")
      .getOrCreate()
    spark.sparkContext.setLogLevel("WARN")
    import spark.implicits._
    val df = spark.readStream
      .format("kafka")
      .option("startingOffsets","earliest")
      .option("kafka.bootstrap.servers", "localhost:9091,localhost:9092,localhost:9093")
      .option("subscribe", "telemetry-replicated").load()
    val str = df.selectExpr("CAST(timestamp as TIMESTAMP)", "CAST(key AS STRING)", "CAST(value AS STRING)")
    val ssd = str.groupBy(window($"timestamp", "10 minutes", "5 minutes")).count
      .writeStream.format("console")
      .option("truncate","false")
      .outputMode("complete")
      .start()
    //val str = newDf.writeStream.format("csv").option("checkpointLocation", "./tmp/streaming").outputMode("append").start("./kafka-sink")
    ssd.awaitTermination()
  }
}
